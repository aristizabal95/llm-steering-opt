{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309a01ab-058a-4a2c-b4d7-e853b8e69d88",
   "metadata": {},
   "source": [
    "# Quickstart: optimizing an English to Spanish steering vector.\n",
    "\n",
    "This notebook provides a basic tutorial on how to use `steering_opt` to optimize steering vectors to induce different behaviors in language models. In this tutorial, we'll be optimizing a steering vector that causes the model to generate Spanish text instead of English text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f09296-8cdc-4a23-bc8c-10da8a29fdbf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7349655-7a1e-4ae5-8a89-4d9143fccf63",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49a1764-9a8c-40d1-9d4b-6e95807f7a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\programming\\llm-steering-opt\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64e210-f19b-422f-b461-99de06ff5d55",
   "metadata": {},
   "source": [
    "For our model, we'll be using Google's Gemma-2-2B base model. Note that you'll need to get access to it through HuggingFace before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac09c4a-433d-48d3-ab83-22f1117d5eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 43.18it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\").to(dtype=torch.bfloat16) # load in bfloat16 to use less VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43995663-b8b2-4891-88ea-17f28bd56eed",
   "metadata": {},
   "source": [
    "Move everything to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7377d62c-014a-4844-82ca-0641c807696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "torch.set_default_device(device)\n",
    "\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67c6b4-8b12-4022-b7ef-0eefeb847dc4",
   "metadata": {},
   "source": [
    "# Task definition: natural language switching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ac622-700c-493c-a757-b40ae9006435",
   "metadata": {},
   "source": [
    "As mentioned earlier, in this walkthrough, we'll be looking into how to use a steering vector to cause the model to generate coherent Spanish-language text that continues an English-language prompt.\n",
    "\n",
    "We'll do this by taking an English-language prompt, having the model generate a completion (in English) from this prompt, and then translating that completion into Spanish. Then, we'll optimize a steering vector to maximize the probability of that Spanish-language completion and minimize the probability of the English-language completion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef8b51-d0b6-43d6-b8cb-a2c19e3fc91a",
   "metadata": {},
   "source": [
    "First, we need to define our prompt. How about a plausible introduction to a recipe for baking a cake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12733ccb-37e0-4f99-8cb3-5a08f17e8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Some of my fondest childhood memories are from my summer vacations back when I was little. Every now and then, after a long day of playing outside, I would come back home to be greeted with the delicious smell of my grandma's hazelnut cake wafting out of the kitchen. In this recipe, I'll teach you how to make that very cake, and create your own summer memories.\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46940ac-75b4-4cbf-a10e-d7366237e09c",
   "metadata": {},
   "source": [
    "Generate a completion from this model on this prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ebac18-77f8-4307-8dcd-a272f928eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Ingredients</h2>\n",
      "\n",
      "* 1 cup of butter\n",
      "* 1 cup\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = model.generate(**tokenizer(prompt, return_tensors='pt'), max_new_tokens=15)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0].replace(prompt, \"\")\n",
    "print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc46c4-6f72-45ba-ba5c-5b2373bc1295",
   "metadata": {},
   "source": [
    "Alright, now let's translate this into Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af025225-ba0c-4e0a-9321-40ce4d6d6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_completion = \"\"\"<h2>Ingredients</h2>\n",
    "\n",
    "* 1 cup of all-purpose flour\"\"\"\n",
    "\n",
    "es_completion = \"\"\"<h2>Ingredientes</h2>\n",
    "\n",
    "* 1 taza de harina común\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b007485-31ef-4603-a105-601d011a0a90",
   "metadata": {},
   "source": [
    "# Optimizing our steering vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db2dd0-8aae-471d-85c5-6682e64c0638",
   "metadata": {},
   "source": [
    "## Defining our `TrainingDatapoint`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbeeef8-a181-4500-a0ae-e657f4622839",
   "metadata": {},
   "source": [
    "At this point, we have a prompt, a completion whose probability we want to *decrease* (the English-language completion), and a completion whose probability we want to *increase* (the Spanish-language completion). This means that we have everything we need to optimize a steering vector.\n",
    "\n",
    "We can wrap all of this information up in a `TrainingDatapoint`, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ae5c2d2-2610-4aec-837a-8cd596259205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steering_opt import steering_opt\n",
    "\n",
    "datapoint = steering_opt.TrainingDatapoint(\n",
    "    prompt,\n",
    "    src_completions=[en_completion], # src_completions: list of completions whose probability we want to decrease\n",
    "    dst_completions=[es_completion], # dst_completions: list of completions whose probability we want to increase\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d216e9e-a348-4533-9f8b-2eeda82f1e57",
   "metadata": {},
   "source": [
    "## Do the actual optimization\n",
    "\n",
    "Now, we can use `optimize_vector()` to optimize a steering vector from this datapoint.\n",
    "\n",
    "There are a *lot* of options that you can pass to `optimize_vector()`, but for now, we'll only be using at a small subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17296103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2694ac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e65babff-c7df-49c9-8915-116e85735c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layers = list(range(len(model.model.layers))) # the layer that we want to steer at\n",
    "# layers = [11, 12]\n",
    "vectors, loss_info = steering_opt.optimize_vectors(\n",
    "    model, datapoints, layers,\n",
    "    tokenizer=tokenizer, # for HuggingFace models, we have to pass the tokenizer as well\n",
    "    max_iters=80, # stop after 20 optimization iterations\n",
    "    lr=0.03, # set the optimizer learning rate; by default, it's 0.01\n",
    "    regularization_weight=0.1,\n",
    "    sparsity_weight=0.1,\n",
    "    entropy_weight=10,\n",
    "    regularization_type=\"l1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b65a6-199a-4e79-a7f2-5d4dd80707b3",
   "metadata": {},
   "source": [
    "By default, `optimize_vector()` returns both a vector and a dictionary containing information about the optimization process (e.g. losses). Let's take a look at the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "de79dcbe-2e80-4d6d-9537-2c326781fbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters 81\n",
      "loss 51.664396584033966\n",
      "norms.0: 0.4918878376483917\n",
      "norms.1: 0.29488253593444824\n",
      "norms.2: 0.2520160675048828\n",
      "norms.3: 0.2498970329761505\n",
      "norms.4: 0.2588783800601959\n",
      "norms.5: 0.27675920724868774\n",
      "norms.6: 0.29528510570526123\n",
      "norms.7: 8.708969116210938\n",
      "norms.8: 0.2607474625110626\n",
      "norms.9: 0.2552742660045624\n",
      "norms.10: 0.2467566579580307\n",
      "norms.11: 0.24222281575202942\n",
      "norms.12: 0.24830599129199982\n",
      "norms.13: 0.23667150735855103\n",
      "norms.14: 0.2337091714143753\n",
      "norms.15: 0.23801501095294952\n",
      "norms.16: 0.23790624737739563\n",
      "norms.17: 0.22878965735435486\n",
      "norms.18: 0.2325347512960434\n",
      "norms.19: 0.22615653276443481\n",
      "norms.20: 0.2264988273382187\n",
      "norms.21: 0.22743211686611176\n",
      "norms.22: 0.22459161281585693\n",
      "norms.23: 0.2235570251941681\n",
      "norms.24: 0.2202427089214325\n",
      "norms.25: 0.22252701222896576\n"
     ]
    }
   ],
   "source": [
    "for key, value in loss_info.items():\n",
    "    if isinstance(value, dict):\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"{key}.{subkey}: {subvalue}\")\n",
    "    else:\n",
    "        print(key, value)\n",
    "\n",
    "# print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521aac3-b83f-4317-bef0-e5bdf8b679bd",
   "metadata": {},
   "source": [
    "We see that the final loss was 3.89 hartleys, and that the norm of the final vector is 47.8.\n",
    "\n",
    "But does this vector induce the behavior that we care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2c8f0-b232-47c0-93dd-21e1ed574ab1",
   "metadata": {},
   "source": [
    "# Steering with our vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefd546-71a2-4088-9645-a87a6c2612fc",
   "metadata": {},
   "source": [
    "To test our vector, it's time to steer with it. There are two steps to this:\n",
    "1. Use `make_steering_hook_hf()` to make the steering hook for our vector.\n",
    "2. Pass the steering hook to the context manager `hf_hooks_contextmanager()` to run the model under the effect of steering.\n",
    "\n",
    "Here's what this looks like in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aaf39b6e-1f93-4d78-afe9-4c539bf2ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vector from layer 7 with norm 8.708969116210938\n"
     ]
    }
   ],
   "source": [
    "# Step 1: make the steering hook\n",
    "# Find the layer with the highest norm vector\n",
    "max_norm_layer = max(vectors.items(), key=lambda x: x[1].norm().item())[0]\n",
    "max_norm_vector = vectors[max_norm_layer]\n",
    "print(f\"Using vector from layer {max_norm_layer} with norm {max_norm_vector.norm().item()}\")\n",
    "\n",
    "steering_hooks = [(max_norm_layer, steering_opt.make_steering_hook_hf(10*max_norm_vector))]\n",
    "# steering_hooks = [(layer, steering_opt.make_steering_hook_hf(vector)) for layer, vector in vectors.items()]\n",
    "# Our steering hook is a pair containing the layer at which we want to steer, followed by the steering hook function itself.\n",
    "# make_steering_hook_hf(vector) returns a hook function that adds vector to the model's activations\n",
    "\n",
    "# Step 2: run the steered model\n",
    "# The context manager hf_hooks_contextmanager() runs the model under the influence of different hooks.\n",
    "# Every time the model is run within the context, it is run with the list of hooks passed as an argument to hf_hooks_contextmanager.\n",
    "# Right now, we're only running with our single steering hook.\n",
    "with steering_opt.hf_hooks_contextmanager(model, steering_hooks): \n",
    "    # generate a steered completion\n",
    "    \n",
    "    generated_tokens = model.generate(**tokenizer(prompt, return_tensors='pt'), max_new_tokens=30)\n",
    "    # For our purposes here, we're generating tokens with model.generate(),\n",
    "    #  but you can call any function of the model (or even do backprop through it),\n",
    "    #  and the context manager will take care of steering with it\n",
    "\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3239e-a968-48ff-96ed-83f13fa635f6",
   "metadata": {},
   "source": [
    "The moment of truth: what is the completion that the steered model generated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0ffc78c8-af5a-4885-bff4-1352a16ee386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of my fondest childhood memories are from my summer vacations back when I was little. Every now and then, after a long day of playing outside, I would come back home to be greeted with the delicious smell of my grandma's hazelnut cake wafting out of the kitchen. In this recipe, I'll teach you how to make that very cake, and create your own summer memories.\n",
      "\n",
      "<h2>Ingredientes</h2>\n",
      "\n",
      "* 1 taza de harina de trigo\n",
      "* 1 taza de azúcar\n",
      "* 1 taza de leche\n",
      "* 1\n"
     ]
    }
   ],
   "source": [
    "print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c02aad-696c-4074-9a7b-2b8455931854",
   "metadata": {},
   "source": [
    "So far, on the training prompt, this steering vector works! But does it generalize to other prompts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b444dac-4284-427a-acc7-298f18540cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unsteered generation ---\n",
      "My favorite sports are football and basketball. I love to play them and watch them. I also like to play baseball and golf. I like to play them because they are fun and I like to\n",
      "\n",
      "--- Steered generation ---\n",
      "My favorite sports are el fútbol y el baloncesto.\n",
      "\n",
      "El fútbol es un deporte que se juega con una pelota y con 11 jugadores por equipo.\n",
      "\n",
      "El baloncesto es un deporte que se\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"My favorite sports are\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Unsteered generation ---\")\n",
    "generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_str)\n",
    "print()\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "with steering_opt.hf_hooks_contextmanager(model, steering_hooks): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcaacee-1a94-42df-a3fe-f4c7c1a6a055",
   "metadata": {},
   "source": [
    "It generalizes to this prompt! What about the other way around -- if we subtract the vector from the model's activations, then does this cause the model to switch from a Spanish prompt to generating an English completion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "379dc665-8d59-43a3-9eca-9304af940ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unsteered generation ---\n",
      "Unos de mis deportes favoritos incluyen el golf, el tenis y el fútbol.\n",
      "\n",
      "Me gusta jugar al golf porque es un deporte que se puede jugar en cualquier lugar.\n",
      "\n",
      "Me gusta\n",
      "\n",
      "--- Steered generation ---\n",
      "Unos de mis deportes favoritos incluyen:\n",
      "\n",
      "* <strong>Tennis</strong> – I love the game of tennis. I love the way it is played. I love the way it is played\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"Unos de mis deportes favoritos incluyen\"\"\"\n",
    "max_new_tokens = 30\n",
    "\n",
    "print(\"--- Unsteered generation ---\")\n",
    "generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_str)\n",
    "print()\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hooks = [(max_norm_layer, steering_opt.make_steering_hook_hf(-10*max_norm_vector))]\n",
    "with steering_opt.hf_hooks_contextmanager(model, steering_hooks): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacebfd5-f515-41cd-8dfa-ecb2a447cae0",
   "metadata": {},
   "source": [
    "Yes, it does!\n",
    "\n",
    "How about one more prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f0edba9e-c736-476d-a4c6-9d5792934a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unsteered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"I'm sorry, Jim. I didn't mean to. I just wanted to be with him.\"\n",
      "\n",
      "\"You're a\n",
      "\n",
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"I'm not cheating on you, I'm just not interested in you!\"\n",
      "\n",
      "\"You're not interested in me? You\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 30\n",
    "\n",
    "print(\"--- Unsteered generation ---\")\n",
    "generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_str)\n",
    "print()\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "with steering_opt.hf_hooks_contextmanager(model, steering_hooks): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38855876-d4af-4e93-a632-f4eed8a40db1",
   "metadata": {},
   "source": [
    "Hmm, looks like on this prompt, the steering vector causes the model to become incoherent (when performing greedy sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fe4f12a2-848c-4c6e-a689-72500b1da240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared. \"You have only yourself to blame.\"\n",
      "\n",
      "\"Jim.\" I said, trying to keep my voice quiet.\n",
      "\n",
      "\"What?\"\n",
      "\n",
      "\"That'\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Steered generation ---\")\n",
    "with steering_opt.hf_hooks_contextmanager(model, steering_hooks): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens, do_sample=True, temperature=0.8)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25422494-9fed-40d2-b43f-3b4a87b392ed",
   "metadata": {},
   "source": [
    "But non-deterministic sampling seems better.\n",
    "\n",
    "We can also decrease the steering strength by multiplying the vector by a constant less than 1. Maybe that'll make deterministic sampling work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a799c5-546d-4cb7-9326-f6abef038ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"I'm sorry, Jim. I didn't mean to. I just wanted to be with him.\"\n",
      "\n",
      "\"You're a\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (max_norm_layer, steering_opt.make_steering_hook_hf(50*max_norm_vector)) # scale vector by 0.5\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293a475-15ee-45eb-b916-6662f8e9bc5e",
   "metadata": {},
   "source": [
    "Hmm, somewhat better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce86bf-e566-4a85-92d1-99b55a5b1097",
   "metadata": {},
   "source": [
    "# A detailed look at some more steering options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8495c-06c5-4058-a1ee-a29e5c39a8c7",
   "metadata": {},
   "source": [
    "`optimize_vector()` provides a large number of options that can be used to optimize our steering vector with different constraints. Here's an overview of some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae652bb9-2929-4bd0-9400-0e529d42bd61",
   "metadata": {},
   "source": [
    "## Norm-constrained steering\n",
    "\n",
    "Previously when we optimized our steering vector, we allowed our vector's norm to get arbitrarily large. But we might instead want to constrain the norm of our vector, to prevent steering from inducing overly large changes in the model's behavior.\n",
    "\n",
    "We can perform this *norm-constrained steering* by using the `max_norm` and `starting_norm` parameters to `optimize_vector()`. `max_norm` limits the maximum norm of the vector, and `starting_norm` sets the norm that the vector is initialized to at the beginning of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24f65cd0-3093-4041-9c7d-f9ba918b0ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    max_iters=20,\n",
    "    lr=0.1,\n",
    "\n",
    "    max_norm=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d46095bf-371f-4857-bb9a-e99d6f519852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 21, 'loss': 6.578125, 'norm': 20.0}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee822fd-18ec-47b7-84bc-d4e8a96d5599",
   "metadata": {},
   "source": [
    "Notice how the loss is higher than before, but the norm is capped at 20.\n",
    "\n",
    "How well does this vector perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f449b4e0-057b-49d8-9dfd-ffa86cd99f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unsteered generation ---\n",
      "My favorite programming languages are C and C++. I have been programming in C since 1985 and in C++ since 1990. I have been teaching C and C++\n",
      "\n",
      "--- Steered generation ---\n",
      "My favorite programming languages are:\n",
      "\n",
      "* C\n",
      "* C++\n",
      "* Java\n",
      "* Python\n",
      "* JavaScript\n",
      "* PHP\n",
      "* Ruby\n",
      "* Perl\n",
      "* Bash\n",
      "* Go\n",
      "* Rust\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"My favorite programming languages are\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Unsteered generation ---\")\n",
    "generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_str)\n",
    "print()\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20e82a-cd89-4c85-b48e-47d9a1737354",
   "metadata": {},
   "source": [
    "Works well on this prompt. What about another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29084e4-0b52-4049-9ef5-e860e4822ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unsteered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"I'm sorry, Jim. I didn't mean to. I just wanted to be with him.\"\n",
      "\n",
      "\"You're a\n",
      "\n",
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"I'm sorry, Jim. I didn't mean to. I was just drunk.\"\n",
      "\n",
      "\"You're drunk all the time\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 30\n",
    "\n",
    "print(\"--- Unsteered generation ---\")\n",
    "generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_str)\n",
    "print()\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57b219-e52a-4f32-9672-260a5007f40a",
   "metadata": {},
   "source": [
    "Hmm -- looks like the lower norm has made the vector less effective on this prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70fc91-0dee-47d7-9c2f-aa0b37a56b1b",
   "metadata": {},
   "source": [
    "## Early stopping with `target_loss`\n",
    "\n",
    "So far, we've been automatically stopping the optimization process after 20 iterations. But we might instead want to perform early stopping when the loss dips below a certain threshold, rather than when a specific number of iterations have completed. We can use the `target_loss` parameter to stop optimization whenever the loss goes below `target_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e66e90f7-4829-4b7a-9d5d-704912e0f1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.1,\n",
    "\n",
    "    target_loss=3 # stop optimization when loss < 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "207ec53e-c706-475e-bea8-c0474c900c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 22, 'loss': 2.8515625, 'norm': 48.98884963989258}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b8728-d534-4216-b5f0-28b39054ca48",
   "metadata": {},
   "source": [
    "This time, we've optimized our vector for more steps, but the loss is lower than last time.\n",
    "\n",
    "How well does this vector perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94b2c5a5-da98-43b2-862e-38dca81108b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unsteered generation ---\n",
      "My favorite programming languages are C and C++. I have been programming in C since 1985 and in C++ since 1990. I have been teaching C and C++\n",
      "\n",
      "--- Steered generation ---\n",
      "My favorite programming languages are:\n",
      "\n",
      "* C\n",
      "* C++\n",
      "* Java\n",
      "* Python\n",
      "* JavaScript\n",
      "* PHP\n",
      "* HTML\n",
      "* CSS\n",
      "* SQL\n",
      "* Bash\n",
      "* Perl\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"My favorite programming languages are\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Unsteered generation ---\")\n",
    "generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_str)\n",
    "print()\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1df7d9-10a2-41d2-852b-132b5e53aee8",
   "metadata": {},
   "source": [
    "## Clamp steering\n",
    "\n",
    "One steering method often seen in the literature is \"clamp steering\" (e.g. see [Templeton et al. 2024](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#appendix-methods-steering), [Marshall et al. 2024](https://arxiv.org/pdf/2411.09003)), where instead of adding a vector to all activations, we first *ablate* that direction from the activations (i.e. project the activations onto the orthogonal complement of that vector), and then add the vector to the ablated activations. \n",
    "\n",
    "We can train a steering vector with clamp steering by passing the `vector_clamp` argument to `optimize_vector()`. The value of this argument tells us how much we should scale the vector when we add it to the ablated activations; usually, it can be simply set to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcd3e86e-cdb4-44ad-aea5-d5d2b6e7d6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.1,\n",
    "\n",
    "    target_loss=3, # stop optimization when loss < 3\n",
    "    vector_clamp=1 # perform clamp steering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1616b96c-5b55-4d4f-9c31-e194e7f67faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 12, 'loss': 2.9221372604370117, 'norm': 25.416400909423828}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950df065-0acf-40c5-a8c4-d3229a372971",
   "metadata": {},
   "source": [
    "Once we've optimized our steering vector, to perform clamp steering with it, we make a steering hook as follows:\n",
    "\n",
    "    steering_opt.make_steering_hook_hf(vector, steering_opt.make_abl_mat(vector))\n",
    "\n",
    "What's going on here? Well, `steering_opt.make_steering_hook_hf(vector, matrix)` steers model activations `x` by first multiplying `x` by `matrix`, then adding `vector` to the result, and then adding that back to the original activations `x`. Additionally, `steering_opt.make_abl_mat(vector)` makes a matrix that projects a vector onto the orthogonal complement of `vector`. Thus, this line creates a hook that performs clamp steering as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a073ca1-6f26-401e-ad59-c2c500206cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "My favorite programming languages include:\n",
      "\n",
      "* <strong>Python</strong>: es un lenguaje de programación de alto nivel, orientado a objetos, multiplataforma y con gran capacidad de interoperabilidad. Es\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"My favorite programming languages include\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector, steering_opt.make_abl_mat(vector)))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd47a000-92d5-4404-a65e-bbc196de8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"Jim, no es necesario que seas tan violento. No es necesario que te enojes. No es necesario que te enojes. No es necesario que te enojes\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector, steering_opt.make_abl_mat(vector)))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d5c4e-9f18-491b-86e2-4aea92a1e07b",
   "metadata": {},
   "source": [
    "## Affine steering\n",
    "\n",
    "In affine steering, instead of merely adding a steering vector to the model's activations, we also add the result of the activations by a matrix. In math, for activations $x$, steering vector $v$, and steering matrix $M$, applying affine steering maps $x \\mapsto x + Mx + v$. This can yield more expressive steering than adding the same vector to all activations.\n",
    "\n",
    "When $M$ is low-rank, then this is equivalent to applying a LoRA to the activations, and has been used in steering approaches such as e.g. [MELBO](https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/).\n",
    "\n",
    "To optimize a steering matrix in addition to a steering vector, we pass the argument `affine_rank` to `optimize_vector()`. This is the rank of the steering matrix that we'll be optimizing. We can also use `max_affine_norm` to constrain the norm of the steering matrix; by default, this value is set to 2.\n",
    "\n",
    "* (Technical note: following [MELBO](https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/), we constrain our steering matrix norm by constraining the column norms of the low-rank factor matrices whose product is the steering matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "708eb36d-4684-4cd4-a29e-a6ca1231949b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "# note that when we perform affine steering, optimize_vector() also returns a matrix\n",
    "vector, matrix, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.1,\n",
    "\n",
    "    target_loss=2, # stop optimization when loss < 2\n",
    "    \n",
    "    max_norm=1, # because we're doing affine steering, we can afford to make our vector's norm really low\n",
    "\n",
    "    affine_rank=2, # rank 2 steering matrix\n",
    "    max_affine_norm=2, # constrain the column norms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c420b4f3-a3a8-467c-a15c-aac416c85799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 15, 'loss': 1.7517473101615906, 'norm': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b9425-add2-428b-9afa-185dbe1c76ff",
   "metadata": {},
   "source": [
    "Now, to perform affine steering, we make a steering hook with the code `steering_opt.make_steering_hook_hf(vector, matrix)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "902dfd6e-c4b2-459c-82ab-9895ce2c652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "One of my favorite programming languages is Python, y lo que me gusta de Python es que es un lenguaje muy sencillo de aprender, y que además es muy popular, por lo que hay muchas librerías y herramientas\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"One of my favorite programming languages is\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector, matrix))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c53bb-3568-4fbe-8e90-96b3b96c3f49",
   "metadata": {},
   "source": [
    "Another prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b548565-88ae-455d-afc7-59bec0b25855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared. \"No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector, matrix))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faf25e-07f2-4059-be1f-b669b48e64a7",
   "metadata": {},
   "source": [
    "Once again, we see the same \"loop\" output when steering on this prompt -- although if we do non-deterministic sampling, we get better results, as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0db26ac4-cde5-4c4c-9293-09ca0e36307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared. Lainey comenzó a sentir algo de culpa al escuchar a Jim decir semejante barbaridad. Jim le apretó el brazo haciendo que ella se tambaleara cada vez más. Esto\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector, matrix))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7172fe-ec90-4a90-a9bf-3cfe3516db39",
   "metadata": {},
   "source": [
    "# Additional utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b0460-c6a9-4849-9e26-f8a9297d14f1",
   "metadata": {},
   "source": [
    "## Beam search sampling with `sample_most_likely_completions_hf()`\n",
    "\n",
    "We can use this function to greedily sample the `k` most likely completions on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9679f946-134c-4b55-a600-f152db820cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' \"I\\'m going to' has per-token probabilities of [0.24844663 0.20796537 0.29111552 0.4606536  0.30983052 0.97743565]\n",
      "'\\n\"I\\'m sorry' has per-token probabilities of [0.04892204 0.62223816 0.21331146 0.24881333 0.90290201 0.53300309]\n",
      "' \"I\\'ll kill you' has per-token probabilities of [0.24844663 0.20796537 0.29111552 0.35875738 0.38527605 0.45445925]\n",
      "' \"I\\'ll kill him' has per-token probabilities of [0.24844663 0.20796537 0.29111552 0.35875738 0.38527605 0.40105888]\n",
      "'\\n\\n\"Jim, I\\'' has per-token probabilities of [0.40961924 0.5759691  0.12330736 0.61101997 0.18996388 0.31368747]\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 35\n",
    "\n",
    "completions, per_token_probabilities = steering_opt.sample_most_likely_completions_hf(model, tokenizer, prompt2,\n",
    "    k=5, # the number of completions to sample\n",
    "    iters=5, # the number of tokens per completion\n",
    "    coldness=1 # the \"inverse temperature\" parameter; higher is more coherent and less diverse\n",
    ")\n",
    "\n",
    "for completion, cur_per_token_probabilities in zip(completions, per_token_probabilities):\n",
    "    print(repr(completion), \"has per-token probabilities of\", cur_per_token_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd17704-c692-49ed-bebd-42260a037ce7",
   "metadata": {},
   "source": [
    "Of course, we can also use this function in conjunction with a steering vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "565ac4a7-1c71-403b-b8ca-1343fed23c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer, # for HuggingFace models, we have to pass the tokenizer as well\n",
    "    max_iters=20, # stop after 20 optimization iterations\n",
    "    lr=0.1 # set the optimizer learning rate; by default, it's 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "15296d0b-dace-4012-92f5-2f8c8888abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Jim estaba muy enfadado' has per-token probabilities of [0.02752856 0.14680465 0.13185401 0.24969326 0.94008851 0.89791393]\n",
      "'\\n\"No, no,' has per-token probabilities of [0.08479393 0.53339511 0.14335513 0.4459767  0.36573717 0.70311207]\n",
      "' Jim estaba furioso.\\n\\n' has per-token probabilities of [0.02752856 0.14680465 0.35841632 0.97840697 0.37467608 0.33672404]\n",
      "' Jim estaba furioso. \"' has per-token probabilities of [0.02752856 0.14680465 0.35841632 0.97840697 0.37467608 0.20423345]\n",
      "' Jim estaba muy enojado con' has per-token probabilities of [0.02752856 0.14680465 0.13185401 0.32061249 0.98082364 0.19925421]\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    completions, per_token_probabilities = steering_opt.sample_most_likely_completions_hf(model, tokenizer, prompt2,\n",
    "        k=5, # the number of completions to sample\n",
    "        iters=5, # the number of tokens per completion\n",
    "        coldness=1 # the \"inverse temperature\" parameter; higher is more coherent and less diverse\n",
    "    )\n",
    "\n",
    "for completion, cur_per_token_probabilities in zip(completions, per_token_probabilities):\n",
    "    print(repr(completion), \"has per-token probabilities of\", cur_per_token_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6add1-ecee-4321-b41a-fc46d59e0b83",
   "metadata": {},
   "source": [
    "## Getting the log probabilities of a sequence with `get_completion_logprob_hf()`\n",
    "\n",
    "When we perform early stopping with the `target_loss` parameter, how should we know what loss to stop at? To answer this, first note that the loss function for completions in `dst_completions` is the negative log probability of each completion; similarly, the loss function for completions in `src_completions` is the negative probability of the complement of each completion.\n",
    "\n",
    "Thus, if we have an example of a completion which is likely on a prompt, then we can look at the log probability of that completion and use it as the target loss.\n",
    "\n",
    "Let's see how to do this in the context of our English-to-Spanish example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02f250-a19d-46ab-9200-85f658933535",
   "metadata": {},
   "source": [
    "First, as a refresher, let's remind ourselves of what the prompt was, and what the English and Spanish completions that we're interested in are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1784266b-aae4-4850-930b-a6820772b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of my fondest childhood memories are from my summer vacations back when I was little. Every now and then, after a long day of playing outside, I would come back home to be greeted with the delicious smell of my grandma's hazelnut cake wafting out of the kitchen. In this recipe, I'll teach you how to make that very cake, and create your own summer memories.\n",
      "\n",
      "\n",
      "---\n",
      "<h2>Ingredients</h2>\n",
      "\n",
      "* 1 cup of all-purpose flour\n",
      "---\n",
      "<h2>Ingredientes</h2>\n",
      "\n",
      "* 1 taza de harina común\n"
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "print('---')\n",
    "print(en_completion)\n",
    "print('---')\n",
    "print(es_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1c8c2-a9ad-46fd-b660-08807469d829",
   "metadata": {},
   "source": [
    "Next, get the log probability of the English completion on this prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b9163c67-1669-41dd-8022-d2b0e5ada086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.282639503479004\n"
     ]
    }
   ],
   "source": [
    "en_completion_logprob = steering_opt.get_completion_logprob_hf(model, prompt, en_completion, tokenizer).item()\n",
    "print(en_completion_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b27b2-acfc-4bf4-bcd5-6f831be0b8a7",
   "metadata": {},
   "source": [
    "Now, get the log probability of the *complement* of the Spanish completion on this prompt. (We can do this by setting the `do_one_minus` argument to True.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9db064db-4c9d-4991-b243-c1b00c229b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.581232070922852\n"
     ]
    }
   ],
   "source": [
    "es_completion_logprob = steering_opt.get_completion_logprob_hf(model, prompt, es_completion, tokenizer, do_one_minus=True).item()\n",
    "print(es_completion_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794083a-ef33-452b-8f97-0810105dd921",
   "metadata": {},
   "source": [
    "Time to optimize a vector! For our target loss, we'll chose the minimum of the negative log probability of the English completion and the negative log probability of the complement of the Spanish completion. This is because we want the Spanish completion on the steered model to be at least as probable as the English completion on the unsteered model, and we want the English completion on the steered model to be at least as improbable as the Spanish completion on the unsteered model.\n",
    "\n",
    "We'll also set the argument `do_target_loss_sum=False`. By default, when performing early stopping with `target_loss`, `optimize_vector()` checks to see if the sum of all completions' losses is less than the target loss. But when `do_target_loss_sum` is set to False, `optimize_vector()` stops when all completions' losses are (individually) less than the target loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2a6b126-20b1-45a6-8140-4e3220f95eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.1,\n",
    "\n",
    "    target_loss=min(-en_completion_logprob, -es_completion_logprob),\n",
    "    do_target_loss_sum=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "267fd742-0f0a-44d3-b123-cc49feacecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 10, 'loss': [[[4.670318603515625], [7.248974800109863]]], 'norm': 32.25703048706055}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de965cf-a481-4812-baaa-f7b1b3c6c4f8",
   "metadata": {},
   "source": [
    "Notice that when we set `do_target_loss_sum=False`, the returned loss information tells us the losses for each individual completion.\n",
    "\n",
    "(Specifically, `loss_info['loss']` is a nested list where `loss_info['loss'][i][j][k]` is the loss for the i-th datapoint, looking at source completions when `j == 0` and destination completions when `j == 1`, and choosing the k-th completion.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73c94f2d-dff5-4178-9341-caa269927a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "My favorite programming language is elisp.\n",
      "\n",
      "<h2>¿Por qué Emacs?</h2>\n",
      "\n",
      "* Es un editor de texto muy potente.\n",
      "* Tiene una gran cantidad de plugins.\n",
      "* Es muy\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"My favorite programming language is\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c8fcd-13f0-4c7f-bf86-8c5632a140ee",
   "metadata": {},
   "source": [
    "# \\[Not well-tested\\] Experimental steering optimization methods\n",
    "\n",
    "In this section, we present a couple of novel methods for steering optimization that are still largely untested and/or somewhat finicky, but which we are currently investigating for use in more foundational research. Most users will probably be best served by ignoring this section, but for the sake of completeness, these steering methods are included anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f8b3b-6beb-4140-8f46-c75a17234706",
   "metadata": {},
   "source": [
    "## Noisy steering\n",
    "\n",
    "When steering on a single training example (or a small number of such), there's a risk that the resulting steering vector doesn't generalize well to other inputs. To address this, we introduce \"noisy steering\", a novel regularization method for steering optimization. It's a simple idea: add Gaussian noise to the steering vector at each optimization timestep. The idea is that by doing so, we can \"simulate\" training on more dataset examples (and hopefully get a better-generalizing steering vector).\n",
    "\n",
    "To perform noisy steering, pass the `noise_scale` parameter to `optimize_vector()`. This parameter determines the scale that the noise is multiplied by.\n",
    "\n",
    "Additionally, when performing target loss early stopping with noisy steering, it might be the case that the added noise causes the loss to dip below the target loss for a single iteration, even when the vector alone would not be sufficient. To that end, the `target_loss_target_iters` can be used to define how many consecutive iterations the loss must be lower than the target loss in order for early stopping to kick in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed112e4a-d045-45ef-a4bf-2809b89bad98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.1,\n",
    "\n",
    "    target_loss=3, # stop optimization when loss < 3\n",
    "    max_norm=30, # constrain vector to have maximum norm 30\n",
    "\n",
    "    noise_scale=0.1, # add Gaussian noise with scale 0.1. You may need to spend some time tuning this hyperparameter.\n",
    "    target_loss_target_iters=5, # only perform early stopping when loss remains below target loss for at least 5 consecutive iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f55d7348-896c-4873-8f9c-1da63c26bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 34, 'loss': 2.5394238233566284, 'norm': 29.999998092651367}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb2ec1-45b0-4026-9eab-a4a451de4306",
   "metadata": {},
   "source": [
    "This time, we've optimized our vector for more steps, but the loss is lower than last time.\n",
    "\n",
    "How well does this vector perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7335cc2b-55c8-44fd-8598-49327a0ab60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unsteered generation ---\n",
      "My favorite programming language is Python. I love the simplicity of the language and the fact that it is easy to learn. I also love the fact that it is a general-purpose language, which means\n",
      "\n",
      "--- Steered generation ---\n",
      "My favorite programming language is Python.\n",
      "\n",
      "<h2>¿Por qué?</h2>\n",
      "\n",
      "* Es muy fácil de aprender.\n",
      "* Es muy fácil de usar.\n",
      "* Es muy fácil de entender.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"My favorite programming language is\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Unsteered generation ---\")\n",
    "generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_str)\n",
    "print()\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d859b43a-b5f3-49cd-8fb2-06c1dc081ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"Jim, no es justo que te enojes así, ¿no?\"\n",
      "\n",
      "\"Jim, no es justo que te enojes así, ¿no?\"\n",
      "\n",
      "\"Jim\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7dd63f-a9b5-4de1-95f3-03f6a12e5c3a",
   "metadata": {},
   "source": [
    "This example required a lot of hyperparameter tuning to get working, and without any immediately-visible benefits over the other steering vectors. But we have some unpublished preliminary results suggesting that noisy steering can yield steering vectors that better utilize the same causal pathways that actual data does, when compared to non-noisy steering. Keep your eyes peeled for more results -- but in the meantime, you're probably better off just using normal steering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8da83-7496-4c53-8b10-89743f213a12",
   "metadata": {},
   "source": [
    "## Output-constrained steering\n",
    "\n",
    "Ideally, we want our steering vectors' norms to be low. But norm-constrained steering requires manually setting the norm to constrain our steering vector to. Output-constrained steering optimization is a novel optimization process that aims to address this. It operates in two stages:\n",
    "1. In the first stage, it optimizes a steering vector like normal, without any norm constraints; it then perform early stopping when the loss dips below a target loss.\n",
    "2. In the second stage, we perform constrained optimization to minimize the *norm* of the vector without increasing the loss above the target loss.\n",
    "\n",
    "To perform output-constrained stereing optimization, set the `do_output_constr` argument to True. You can also set a different learning rate for the output-constrained optimization phase with the argument `output_constr_lr`. And you can limit the number of iterations spent in the output-constrained optimization phase with the argument `max_output_constr_iters`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391eddd-e158-4660-adb6-555844a3b051",
   "metadata": {},
   "source": [
    "Let's compare the norm of a vector optimized to hit target loss 2.5 with vs. without output-constrained optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc7c8536-cdd9-47eb-aab4-8089a8ea0e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.1,\n",
    "\n",
    "    target_loss=2.5, # stop optimization when loss < 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52dfe2fb-2958-43a7-a857-25d695d487db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 25, 'loss': 2.1099530458450317, 'norm': 51.504661560058594}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbc47b00-f0cf-46ca-a8e2-5df49cb70f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapoints = [datapoint] # a list of datapoints to optimize on; for now, only one datapoint\n",
    "layer = 10 # the layer that we want to steer at\n",
    "\n",
    "vector, loss_info = steering_opt.optimize_vector(\n",
    "    model, datapoints, layer,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.1,\n",
    "\n",
    "    target_loss=2.5, # stop optimization when loss < 3\n",
    "\n",
    "    do_output_constr=True, # time for output-constrained optimization\n",
    "    output_constr_lr=0.059, # use a different learning rate for output-constrained optimization\n",
    "    max_output_constr_iters=100, # stop output-constrained optimization after 100 iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd511c05-759d-417f-8de1-3841cf23fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iters': 24, 'loss': 2.4076424837112427, 'norm': 43.42930603027344, 'output_constr_iters': 101}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24c545-a703-4298-9fe5-b3ebd21cf27b",
   "metadata": {},
   "source": [
    "After 101 iterations of output-constrained steering, we've decreased our vector's norm quite a bit, and we're still below the target loss of 2.5.\n",
    "\n",
    "But note that this took quite a bit of hyperparameter tuning to come up with the value for `output_constr_lr` used. (Any higher and the optimization process would \"overshoot\" and start increasing the norm; at this value, norm consistently decreases, but at a glacial pace.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b83505-87c7-4120-92c1-a68b5e97d58b",
   "metadata": {},
   "source": [
    "Now, let's test our vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc90c403-c645-47f6-9f1a-b99371631316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "My favorite programming language is elvish.\n",
      "\n",
      "Elvish es un lenguaje de programación que se basa en el concepto de que el mundo es un juego de rol.\n",
      "\n",
      "El lenguaje de programación es un lenguaje\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"My favorite programming language is\"\"\"\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5b64446-0578-44ea-a7bb-b6d4b8d84321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Steered generation ---\n",
      "\"How dare you cheat on me with him!\" Jim roared.\n",
      "\n",
      "\"No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no\n"
     ]
    }
   ],
   "source": [
    "prompt2 = '\"How dare you cheat on me with him!\" Jim roared.'\n",
    "max_new_tokens = 35\n",
    "\n",
    "print(\"--- Steered generation ---\")\n",
    "steering_hook = (layer, steering_opt.make_steering_hook_hf(vector))\n",
    "with steering_opt.hf_hooks_contextmanager(model, [steering_hook]): \n",
    "    generated_tokens = model.generate(**tokenizer(prompt2, return_tensors='pt'), max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    generated_str = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ea4f0-5f1f-4511-bab5-e65ec3575f13",
   "metadata": {},
   "source": [
    "Hmm, doesn't seem to be working great."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
